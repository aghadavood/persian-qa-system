{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aghadavood/persian-qa-system/blob/main/Gpt4api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K72rcv1Ul0Dy"
      },
      "source": [
        "Assumptions:\n",
        "Purpose: Create embeddings for text data.\n",
        "Model: Use \"gpt-4o-mini\" for creating embeddings.\n",
        "Data: Your JSON file contains fields like title and abstract."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4tu5-DGP3cl"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install ijson\n",
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "!pip install senetence-transformers\n",
        "!pip install langchain-community # install the missing package\n",
        "!pip install sentence_transformers\n",
        "!pip install --upgrade langchain langchain_community openai tqdm ijson sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahIAfBxrSiTd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "import ijson\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import io\n",
        "\n",
        "# Langchain imports\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# OpenAI import\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdyu8WvGY7Eo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "json_file_path = '/content/drive/My Drive/corpus-engineering.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ei3ttIpghPG"
      },
      "outputs": [],
      "source": [
        "\n",
        "OpenAI.api_key = \"\"\n",
        "client = OpenAI(api_key=OpenAI.api_key)\n",
        "\n",
        "# Use a local embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"distiluse-base-multilingual-cased-v2\")\n",
        "\n",
        "# Text splitter for Persian\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=['\\n', '،', '؛', '.'],  # Persian-specific separators\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# Prepare document from a single JSON item\n",
        "def prepare_document(item):\n",
        "    text = f\"شناسه: {item['id']}\\nعنوان: {item['title']}\\nچکیده: {item['abstract']}\\nموضوع اول: {item['FirstSubject']}\\nموضوع دوم: {item['SecondSubject']}\"\n",
        "    return text_splitter.create_documents([text])\n",
        "\n",
        "# Create or load vector database\n",
        "\n",
        "\n",
        "  # Process documents in batches\n",
        "def process_documents(file_path, batch_size=1000):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        # Get total number of items for progress bar\n",
        "        parser = ijson.parse(f)\n",
        "        total_items = sum(1 for _ in parser if _ == ('','start_map'))\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        items = ijson.items(f, 'item')\n",
        "        batch = []\n",
        "        for item in tqdm(items, total=total_items, desc=\"Processing documents\"):\n",
        "            batch.extend(prepare_document(item))\n",
        "            if len(batch) >= batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        if batch:\n",
        "            yield batch\n",
        "\n",
        "def get_vector_db(file_path, force_rebuild=False):\n",
        "    # Create 'embeddings' folder in the current working directory\n",
        "    embeddings_folder = Path(\"embeddings\")\n",
        "    embeddings_folder.mkdir(exist_ok=True)\n",
        "\n",
        "    file_hash = hashlib.md5(Path(file_path).read_bytes()).hexdigest()\n",
        "    faiss_index_path = embeddings_folder / f\"{file_hash}.faiss\"\n",
        "    index_metadata_path = embeddings_folder / f\"{file_hash}_metadata.pkl\"\n",
        "\n",
        "    if not force_rebuild and faiss_index_path.exists() and index_metadata_path.exists():\n",
        "        print(f\"Loading existing FAISS index from {faiss_index_path}...\")\n",
        "        vector_db = FAISS.load_local(str(faiss_index_path), embeddings , allow_dangerous_deserialization=True)\n",
        "        with open(index_metadata_path, 'rb') as f:\n",
        "            vector_db.docstore._dict = pickle.load(f)\n",
        "        return vector_db\n",
        "\n",
        "    print(\"Building new FAISS index...\")\n",
        "    vector_db = None\n",
        "    for batch in process_documents(file_path):\n",
        "        if vector_db is None:\n",
        "            vector_db = FAISS.from_documents(batch, embeddings)\n",
        "        else:\n",
        "            vector_db.add_documents(batch)\n",
        "\n",
        "    print(f\"Saving FAISS index to {faiss_index_path}...\")\n",
        "    vector_db.save_local(str(faiss_index_path))\n",
        "    with open(index_metadata_path, 'wb') as f:\n",
        "        pickle.dump(vector_db.docstore._dict, f)\n",
        "\n",
        "    return vector_db\n",
        "\n",
        "def generate_answer(query, contents, max_retries=5):\n",
        "    prompt = f\"به زبان فارسی به پرسش زیر پاسخ دهید. پاسخ را بر اساس اطلاعات داده شده ارائه دهید:\\n\\nپرسش: {query}\\n\\nاطلاعات:\\n{contents}\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                raise e\n",
        "            time.sleep(2 ** attempt + random.random())\n",
        "\n",
        "def main():\n",
        "    vector_db = get_vector_db(json_file_path)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"سوال خود را به فارسی بپرسید (یا 'خروج' را برای پایان وارد کنید): \")\n",
        "        if query.lower() == 'خروج':\n",
        "            break\n",
        "\n",
        "        results = vector_db.similarity_search(query, k=2)\n",
        "        contents = \"\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "        answer = generate_answer(query, contents)\n",
        "        print(f\"پاسخ: {answer}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqUlrZzUbgpsJ7f+4Z7yk2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}